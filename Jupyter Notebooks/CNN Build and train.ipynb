{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8435637f",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55950ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import cv2 as cv\n",
    "from skimage.filters import threshold_otsu, threshold_niblack, threshold_sauvola\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.models import model_from_json, Sequential\n",
    "from tensorflow.keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Activation, Input, concatenate\n",
    "from tensorflow.keras.layers import Layer, BatchNormalization, Conv2D, MaxPooling2D, Concatenate, Lambda, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.initializers import glorot_uniform, he_uniform\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import math\n",
    "\n",
    "import json\n",
    "\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dcffcf",
   "metadata": {},
   "source": [
    "### Load previously processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bcc2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = np.load('fingers_poly.npz')\n",
    "x_train, y_train, x_test, y_test = loaded['x_train'], loaded['y_train'], loaded['x_test'], loaded['y_test']\n",
    "\n",
    "x_train = x_train/255.\n",
    "x_test = x_test/255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec423d",
   "metadata": {},
   "source": [
    "Some cells below take x_train.shape as input to create the CNN. If you do not have the dataset but want to create the architecture and then load the pretrained weights know that x_train.shape is: (6, 336, 356, 328) (It can also be seen in the \"Preprocess data from poly Dataset.ipynb\") so just create a zero np.array of these dimensions and the model will be compiled correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4cdea",
   "metadata": {},
   "source": [
    "### Create the triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fdb8d",
   "metadata": {},
   "source": [
    "Two functions will be implemented. create_batch will randomly select triplets from the dataset. create_hard_batch will add some semi-hard triplets to every batch. Semi-hard triplets are the ones where the negative example is farther away from the anchor than the positive example but still produces positive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a6fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_hard = 16\n",
    "def create_batch(batch_size = batch_size, split = 'train'):\n",
    "    \n",
    "    x_anchors = np.zeros((batch_size, x_train.shape[2], x_train.shape[3]))\n",
    "    x_positives = np.zeros((batch_size, x_train.shape[2], x_train.shape[3]))\n",
    "    x_negatives = np.zeros((batch_size, x_train.shape[2], x_train.shape[3]))\n",
    "    \n",
    "    if split ==\"train\":\n",
    "        data = x_train\n",
    "        data_y = y_train\n",
    "    else:\n",
    "        data = x_test\n",
    "        data_y = y_test\n",
    "    \n",
    "    for i in range(0, batch_size):\n",
    "        # Find an anchor, a positive (augmentation from the same finger) and a negative example.\n",
    "        # anchor\n",
    "        rand_finger = random.randint(0, data.shape[1] - 1) # Choose a finger randomly\n",
    "        x_anchor = data[0, rand_finger, :, :]\n",
    "        \n",
    "        # positive\n",
    "        rand_aug = random.randint(1, 5)                    # Choose a random augmentation of it\n",
    "        while np.all(data[rand_aug, rand_finger, :, :] == np.zeros(shape=(x_train.shape[2],x_train.shape[3]))):\n",
    "            rand_aug = random.randint(1, 5)\n",
    "        x_positive = data[rand_aug, rand_finger, :, :]\n",
    "        \n",
    "        # negative\n",
    "        rand_neg = random.randint(0, data.shape[1] - 1)    # Choose a random negative finger DIFFERENT than the anchor\n",
    "        while rand_neg == rand_finger:\n",
    "            rand_neg = random.randint(0, data.shape[1] - 1)\n",
    "        x_negative = data[0, rand_neg, :, :]\n",
    "        \n",
    "        x_anchors[i, :, :] = x_anchor\n",
    "        x_positives[i, :, :] = x_positive\n",
    "        x_negatives[i, :, :] = x_negative\n",
    "        \n",
    "    return [x_anchors, x_positives, x_negatives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd24dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hard_batch(batch_size, num_hard, split = 'train'):\n",
    "    \n",
    "    x_anchors = np.zeros((batch_size, x_train.shape[2], x_train.shape[3]))\n",
    "    x_positives = np.zeros((batch_size, x_train.shape[2], x_train.shape[3]))\n",
    "    x_negatives = np.zeros((batch_size, x_train.shape[2], x_train.shape[3]))\n",
    "    \n",
    "    if split ==\"train\":\n",
    "        data = x_train\n",
    "        data_y = y_train\n",
    "    else:\n",
    "        data = x_test\n",
    "        data_y = y_test\n",
    "    \n",
    "    # Generate num_hard number of hard examples\n",
    "    hard_batches = []\n",
    "    batch_losses = []\n",
    "    \n",
    "    rand_batches = []\n",
    "    \n",
    "    # Get some random batches\n",
    "    for i in range(0, batch_size):\n",
    "        hard_batches.append(create_batch(1, split)) # Returns only one triplet\n",
    "        \n",
    "        A_emb = embedding_model.predict(np.expand_dims(hard_batches[i][0], axis = -1))\n",
    "        P_emb = embedding_model.predict(np.expand_dims(hard_batches[i][1], axis = -1))\n",
    "        N_emb = embedding_model.predict(np.expand_dims(hard_batches[i][2], axis = -1))\n",
    "        \n",
    "        # Compute distance for each selected batch\n",
    "        batch_losses.append(np.sum(np.square(A_emb - P_emb), axis = 1) - np.sum(np.square(A_emb - N_emb), axis = 1))\n",
    "        \n",
    "    # Sort batch_loss by distance, highest first, and keep num_hard of them\n",
    "    \n",
    "    # Semi-hard require dist(A,N) > dist(A,P) AND still positive loss so\n",
    "    # I choose the hard batches based on batch loss ascending order. The line\n",
    "    # below creates zip/pairs of batch_losses, hard_batches and returns ONLY\n",
    "    # the hard batches (_,x) having sorted in ascending order the batch loss (x[0]).\n",
    "    \n",
    "    hard_batch_selections = [x for _, x in sorted(zip(batch_losses, hard_batches), key = lambda x: x[0])]\n",
    "    hard_batches = hard_batch_selections[:num_hard]\n",
    "    \n",
    "    # Get batch_size - num_hard number of random examples\n",
    "    num_rand = batch_size - num_hard\n",
    "    for i in range(0, num_rand):\n",
    "        rand_batch = create_batch(1, split)\n",
    "        rand_batches.append(rand_batch)\n",
    "        \n",
    "    selections = hard_batches + rand_batches\n",
    "    \n",
    "    for i in range(0, len(selections)):\n",
    "        x_anchors[i, :, :] = selections[i][0]\n",
    "        x_positives[i, :, :] = selections[i][1]\n",
    "        x_negatives[i, :, :] = selections[i][2]\n",
    "\n",
    "    x_anchors = np.expand_dims(x_anchors, axis = -1)\n",
    "    x_positives = np.expand_dims(x_positives, axis = -1)\n",
    "    x_negatives = np.expand_dims(x_negatives, axis = -1)\n",
    "\n",
    "    return [x_anchors, x_positives, x_negatives]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b6b7f",
   "metadata": {},
   "source": [
    "### The SNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3bf5b8",
   "metadata": {},
   "source": [
    "The SNN is defined in two parts. First the embedding model: Input: image, Output: d-dimensional embedding. Then a model takes a triplet and passes it sequentially through the embedding model to generate 3 embeddings which are then fed into the triplet loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "624ad3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_model(emb_size):\n",
    "    \n",
    "    embedding_model = tf.keras.models.Sequential()\n",
    "    \n",
    "    embedding_model.add(Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same', \n",
    "                     activation = 'relu', kernel_initializer='he_uniform',\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.001), input_shape = (x_train.shape[2] , x_train.shape[3], 1)))\n",
    "    \n",
    "    embedding_model.add(BatchNormalization())\n",
    "    \n",
    "    embedding_model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2))) # output: (178,164,32)\n",
    "    \n",
    "    embedding_model.add(Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same', \n",
    "                     activation = 'relu', kernel_initializer='he_uniform',\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    \n",
    "    embedding_model.add(BatchNormalization())\n",
    "    \n",
    "    embedding_model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2))) # output: (89,82,64)\n",
    "    \n",
    "    embedding_model.add(Conv2D(filters = 128, kernel_size = (3,3), strides = (1,1), padding = 'same', \n",
    "                     activation = 'relu', kernel_initializer='he_uniform',\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    \n",
    "    embedding_model.add(BatchNormalization())\n",
    "    \n",
    "    embedding_model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2))) # output: (44,41,128)\n",
    "    \n",
    "    embedding_model.add(Conv2D(filters = 256, kernel_size = (3,3), strides = (1,1), padding = 'same', \n",
    "                     activation = 'relu', kernel_initializer='he_uniform',\n",
    "                     kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    \n",
    "    embedding_model.add(BatchNormalization())\n",
    "    \n",
    "    embedding_model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2))) # output: (22,20,256)\n",
    "    \n",
    "    embedding_model.add(Dropout(rate = 0.3))\n",
    "    \n",
    "    embedding_model.add(Flatten()) # output: 22*20*256\n",
    "    \n",
    "    embedding_model.add(Dense(units = 512, activation = 'relu', kernel_initializer='he_uniform',\n",
    "             kernel_regularizer=tf.keras.regularizers.l2(1e-3)))\n",
    "    \n",
    "    embedding_model.add(Dense(units = 256, activation = 'relu', kernel_initializer='he_uniform',\n",
    "             kernel_regularizer=tf.keras.regularizers.l2(1e-3)))\n",
    "    \n",
    "    # output layer with embedding_size no of units\n",
    "    embedding_model.add(Dense(units = emb_size, activation = None, kernel_initializer='he_uniform',\n",
    "             kernel_regularizer=tf.keras.regularizers.l2(1e-3)))\n",
    "    \n",
    "    # Force the embedding to live on the d-dimentional hypershpere\n",
    "    embedding_model.add(Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=-1)))\n",
    "    \n",
    "    embedding_model.summary()\n",
    "    \n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b77952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_SNN(embedding_model):\n",
    "    \n",
    "    input_anchor = Input(shape = (x_train.shape[2] , x_train.shape[3] ,))\n",
    "    input_positive = Input(shape = (x_train.shape[2] , x_train.shape[3] ,))\n",
    "    input_negative = Input(shape = (x_train.shape[2] , x_train.shape[3] ,))\n",
    "    \n",
    "    embedding_anchor = embedding_model(input_anchor)\n",
    "    embedding_positive = embedding_model(input_positive)\n",
    "    embedding_negative = embedding_model(input_negative)\n",
    "    \n",
    "    output = concatenate([embedding_anchor, embedding_positive, embedding_negative], axis = 1)\n",
    "    \n",
    "    siamese_net = tf.keras.models.Model(inputs = [input_anchor, input_positive, input_negative],\n",
    "                                        outputs = output)\n",
    "    \n",
    "    siamese_net.summary()\n",
    "    \n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb2c2cb",
   "metadata": {},
   "source": [
    "### Triplet Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bdadc",
   "metadata": {},
   "source": [
    "In order for the SNN to train using the triplets, we need to define the triplet loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21050e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred):\n",
    "    anchor, positive, negative = y_pred[:, :emb_size], y_pred[:, emb_size:2*emb_size], y_pred[:, 2*emb_size:]\n",
    "    positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis = 1)\n",
    "    negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis = 1)\n",
    "    tr_loss = tf.maximum(positive_dist - negative_dist + alpha, 0.)\n",
    "    return tr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094eec9e",
   "metadata": {},
   "source": [
    "### Data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5afb6e",
   "metadata": {},
   "source": [
    "In order to pass the triplets to the network, we need to create a data generator function. Both an x and y is required here by TensorFlow, but we don’t need a y value, so we pass a filler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82fca649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size = batch_size, num_hard = num_hard, split = 'train'):\n",
    "    leles=0 # leles is a variable to display the progress of each epoch in training\n",
    "    while True:\n",
    "        x = create_hard_batch(batch_size, num_hard)\n",
    "        y = np.zeros((batch_size, 3*emb_size))\n",
    "        leles+=1\n",
    "        if leles%6==0:\n",
    "            print(f'',int(leles/6*20),'%')\n",
    "        if leles==30:\n",
    "            leles=0\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e5daf",
   "metadata": {},
   "source": [
    "### Setup for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6173c2",
   "metadata": {},
   "source": [
    "1. Define hyperparameters\n",
    "2. Compile models\n",
    "\n",
    "Once the models are compiled, we store a subset of the test image embeddings. The model hasn’t been trained yet, so this gives us a good baseline to show how the embeddings have changed through the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc0272f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding model... \n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 356, 328, 32)      320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 356, 328, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 178, 164, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 178, 164, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 178, 164, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 89, 82, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 89, 82, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 89, 82, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 44, 41, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 44, 41, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 44, 41, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 22, 20, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 20, 256)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 112640)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               57672192  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 128)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,226,176\n",
      "Trainable params: 58,225,216\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "\n",
      " Generating SNN... \n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 356, 328)]   0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 356, 328)]   0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 356, 328)]   0           []                               \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 128)          58226176    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 384)          0           ['sequential_1[0][0]',           \n",
      "                                                                  'sequential_1[1][0]',           \n",
      "                                                                  'sequential_1[2][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 58,226,176\n",
      "Trainable params: 58,225,216\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "steps_per_epoch = 30\n",
    "val_steps = 10\n",
    "alpha = 0.3\n",
    "num_hard = 16\n",
    "lr = 0.0001\n",
    "optimiser = 'Adam'\n",
    "emb_size = 128\n",
    "\n",
    "with tf.device('GPU'):\n",
    "    # Create the embedding model\n",
    "    print('Generating embedding model... \\n')\n",
    "    embedding_model = create_embedding_model(emb_size)\n",
    "    \n",
    "    print('\\n Generating SNN... \\n')\n",
    "    # Create the SNN\n",
    "    siamese_net = create_SNN(embedding_model)\n",
    "    # Compile the SNN\n",
    "    optimiser_obj = Adam(lr = lr)\n",
    "    siamese_net.compile(loss = triplet_loss, optimizer = optimiser_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76852790",
   "metadata": {},
   "source": [
    "### Logging output from our model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ce3cc",
   "metadata": {},
   "source": [
    "The script below will save the model in a folder named \"name\" (first line). After each epoch, if the val_loss haw improved, the previous model will be deleted and the new will be saved. The folder contains both the SNN and the embedding model alone, which is the one used for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60a9a674",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Το σύστημα δεν είναι σε θέση να εντοπίσει την καθορισμένη διαδρομή δίσκου: '/home/giorgossykas\\\\CustomCNN_final'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m logdir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/giorgossykas\u001b[39m\u001b[38;5;124m'\u001b[39m,name) \u001b[38;5;66;03m# Change /home/giorgossykas to your directory\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(logdir):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Callbacks:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create the TensorBoard callback\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tensorboard \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(\n\u001b[0;32m     10\u001b[0m     log_dir \u001b[38;5;241m=\u001b[39m logdir,\n\u001b[0;32m     11\u001b[0m     histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     profile_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     18\u001b[0m )\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Το σύστημα δεν είναι σε θέση να εντοπίσει την καθορισμένη διαδρομή δίσκου: '/home/giorgossykas\\\\CustomCNN_final'"
     ]
    }
   ],
   "source": [
    "name = \"CustomCNN_final\"\n",
    "logdir = os.path.join(r'/home/giorgossykas',name) # Change /home/giorgossykas to your directory\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.mkdir(logdir)\n",
    "    \n",
    "# Callbacks:\n",
    "# Create the TensorBoard callback\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir = logdir,\n",
    "    histogram_freq=0,\n",
    "    batch_size=batch_size,\n",
    "    write_graph=True,\n",
    "    write_grads=True, \n",
    "    write_images = True, \n",
    "    update_freq = 'epoch', \n",
    "    profile_batch=0\n",
    ")\n",
    "\n",
    "# Training logger\n",
    "csv_log = os.path.join(logdir, 'training.csv')\n",
    "csv_logger = CSVLogger(csv_log, separator=',', append=True)\n",
    "\n",
    "# Only save the best model weights based on the val_loss\n",
    "checkpoint = ModelCheckpoint(os.path.join(logdir, 'snn_model-{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "                             monitor='val_loss', verbose=1,\n",
    "                             save_best_only=True, save_weights_only=True, \n",
    "                             mode='auto')\n",
    "\n",
    "# Save the embedding model weights based on the main model's val loss\n",
    "# This is needed to recreate the emebedding model should we wish to visualise\n",
    "# the latent space at the saved epoch\n",
    "\n",
    "class SaveEmbeddingModelWeights(Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', verbose=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.best = np.Inf\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"SaveEmbeddingModelWeights requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current < self.best:\n",
    "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
    "            if self.verbose == 1:\n",
    "                print(\"Saving embedding model weights at %s\" % filepath)\n",
    "            embedding_model.save_weights(filepath, overwrite = True)\n",
    "            self.best = current\n",
    "            \n",
    "            # Delete the last best emb_model and snn_model\n",
    "            delete_older_model_files(filepath)\n",
    "            \n",
    "# Save the embedding model weights if you save a new snn best model based on the model checkpoint above\n",
    "emb_weight_saver = SaveEmbeddingModelWeights(os.path.join(logdir, 'emb_model-{epoch:02d}.h5'))\n",
    "\n",
    "callbacks = [tensorboard, csv_logger, checkpoint, emb_weight_saver]\n",
    "\n",
    "# Save model configs to JSON\n",
    "model_json = siamese_net.to_json()\n",
    "with open(os.path.join(logdir, \"siamese_config.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    json_file.close()\n",
    "    \n",
    "model_json = embedding_model.to_json()\n",
    "with open(os.path.join(logdir, \"embedding_config.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    json_file.close()\n",
    "    \n",
    "hyperparams = {'batch_size' : batch_size,\n",
    "              'epochs' : epochs, \n",
    "               'steps_per_epoch' : steps_per_epoch, \n",
    "               'val_steps' : val_steps, \n",
    "               'alpha' : alpha, \n",
    "               'num_hard' : num_hard, \n",
    "               'optimiser' : optimiser,\n",
    "               'lr' : lr,\n",
    "               'emb_size' : emb_size\n",
    "              }\n",
    "\n",
    "with open(os.path.join(logdir, \"hyperparams.json\"), \"w\") as json_file:\n",
    "    json.dump(hyperparams, json_file)\n",
    "    \n",
    "# Set the model to TB\n",
    "tensorboard.set_model(siamese_net)\n",
    "\n",
    "def delete_older_model_files(filepath):\n",
    "    \n",
    "    model_dir = filepath.split(\"emb_model\")[0]\n",
    "    \n",
    "    # Get model files\n",
    "    model_files = os.listdir(model_dir)\n",
    "    # Get only the emb_model files\n",
    "    emb_model_files = [file for file in model_files if \"emb_model\" in file]\n",
    "    # Get the epoch nums of the emb_model_files\n",
    "    emb_model_files_epoch_nums = [file.split(\"-\")[1].split(\".h5\")[0] for file in emb_model_files]\n",
    "\n",
    "    # Find all the snn model files\n",
    "    snn_model_files = [file for file in model_files if \"snn_model\" in file]\n",
    "\n",
    "    # Sort, get highest epoch num\n",
    "    emb_model_files_epoch_nums.sort()\n",
    "    highest_epoch_num = emb_model_files_epoch_nums[-1]\n",
    "\n",
    "    # Filter the emb_model and snn_model file lists to remove the highest epoch number ones\n",
    "    emb_model_files_without_highest = [file for file in emb_model_files if highest_epoch_num not in file]\n",
    "    snn_model_files_without_highest = [file for file in snn_model_files if highest_epoch_num not in file]\n",
    "\n",
    "    # Delete the non-highest model files from the subdir\n",
    "    if len(emb_model_files_without_highest) != 0:\n",
    "        print(\"Deleting previous best model file:\", emb_model_files_without_highest)\n",
    "        for model_file_list in [emb_model_files_without_highest, snn_model_files_without_highest]:\n",
    "            for file in model_file_list:\n",
    "                os.remove(os.path.join(model_dir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe4d95",
   "metadata": {},
   "source": [
    "### Training the SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# siamese_net is already compiled\n",
    "\n",
    "siamese_history = siamese_net.fit(\n",
    "    data_generator(batch_size, num_hard),\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs = epochs,\n",
    "    verbose = 2,\n",
    "    callbacks = callbacks,\n",
    "    workers = 0,\n",
    "    validation_data = data_generator(batch_size, num_hard, split = 'test'),\n",
    "    validation_steps = val_steps)\n",
    "\n",
    "print('-------------------------------------------')\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad784dc",
   "metadata": {},
   "source": [
    "Now the final model is saved in the folder and can be used to predict. Both siamese net and embedding model are saved but only the embedding will be used for predicting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingerprint",
   "language": "python",
   "name": "fingerprint"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
